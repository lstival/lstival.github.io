Title: Publications
Slug: publications

# Publications

Here you'll find a selection of my research publications. Click on the titles to access the full papers, and use the BibTeX citations for your references.

---

## 2025

### Deep Learning Approaches for Self-Supervised Representation Learning

**Conference on Advanced Machine Learning 2025**

**Authors:** Your Name, Collaborator A, Collaborator B

This paper introduces a novel self-supervised learning approach that leverages contrastive learning to improve representation quality across multiple domains.

#### Abstract

This paper introduces a novel self-supervised learning approach that leverages contrastive learning to improve representation quality and generalization across multiple domains. We demonstrate state-of-the-art performance on several benchmark datasets.

**Pages:** 123â€“145  
**Publisher:** Springer

[ðŸ“„ Download PDF](#) | [ðŸ”— arXiv](#) | [ðŸ’» Code](#)

##### BibTeX Citation

```bibtex
@inproceedings{yourname2025deep,
  title={Deep Learning Approaches for Self-Supervised Representation Learning},
  author={Your Name and Collaborator A and Collaborator B},
  booktitle={Conference on Advanced Machine Learning},
  pages={123--145},
  year={2025},
  organization={Springer}
}
```

---

### Multimodal Learning with Attention Mechanisms

**Journal of Artificial Intelligence Research 2025**

**Authors:** Your Name, Collaborator C

This work explores attention-based architectures for multimodal learning, achieving improved performance on cross-modal retrieval tasks.

#### Abstract

This work explores attention-based architectures for multimodal learning, demonstrating how cross-attention mechanisms can effectively align representations from different modalities. Our approach achieves improved performance on cross-modal retrieval and understanding tasks.

**Volume:** 42  
**Pages:** 567â€“589  
**Publisher:** AAAI Press

[ðŸ“„ Download PDF](#) | [ðŸ”— DOI](#)

##### BibTeX Citation

```bibtex
@article{yourname2025multimodal,
  title={Multimodal Learning with Attention Mechanisms},
  author={Your Name and Collaborator C},
  journal={Journal of Artificial Intelligence Research},
  volume={42},
  pages={567--589},
  year={2025},
  publisher={AAAI Press}
}
```

---

## 2024

### Transformer-Based Models for Time Series Analysis

**International Conference on Machine Learning 2024**

**Authors:** Your Name, Collaborator D, Collaborator E

We present a transformer-based architecture specifically designed for time series forecasting and anomaly detection.

#### Abstract

We present a transformer-based architecture specifically designed for time series forecasting and anomaly detection. By incorporating temporal positional encodings and specialized attention mechanisms, our model captures long-range dependencies more effectively than traditional approaches.

**Pages:** 2341â€“2356  
**Publisher:** PMLR

[ðŸ“„ Download PDF](#) | [ðŸ”— arXiv](#) | [ðŸ’» Code](#)

##### BibTeX Citation

```bibtex
@inproceedings{yourname2024transformer,
  title={Transformer-Based Models for Time Series Analysis},
  author={Your Name and Collaborator D and Collaborator E},
  booktitle={International Conference on Machine Learning},
  pages={2341--2356},
  year={2024},
  organization={PMLR}
}
```

---

### Contrastive Learning for Domain Adaptation

**Neural Information Processing Systems 2024**

**Authors:** Your Name, Collaborator F

This paper investigates contrastive learning strategies for effective domain adaptation in computer vision tasks.

#### Abstract

This paper investigates contrastive learning strategies for effective domain adaptation in computer vision tasks. We propose a novel framework that aligns source and target domain representations while preserving discriminative information.

**Pages:** 8912â€“8925  
**Publisher:** NeurIPS

[ðŸ“„ Download PDF](#) | [ðŸ”— arXiv](#)

##### BibTeX Citation

```bibtex
@inproceedings{yourname2024contrastive,
  title={Contrastive Learning for Domain Adaptation},
  author={Your Name and Collaborator F},
  booktitle={Neural Information Processing Systems},
  pages={8912--8925},
  year={2024},
  organization={NeurIPS}
}
```

---

## 2023

### Survey on Modern Deep Learning Architectures

**ACM Computing Surveys 2023**

**Authors:** Your Name, Collaborator G, Collaborator H

A comprehensive survey covering recent advances in deep learning architectures, from CNNs to transformers and beyond.

#### Abstract

A comprehensive survey covering recent advances in deep learning architectures, from CNNs to transformers and beyond. We provide a unified perspective on architectural design principles and their applications across different domains.

**Volume:** 55  
**Issue:** 4  
**Pages:** 1â€“38  
**Publisher:** ACM

[ðŸ“„ Download PDF](#) | [ðŸ”— DOI](#)

##### BibTeX Citation

```bibtex
@article{yourname2023survey,
  title={Survey on Modern Deep Learning Architectures},
  author={Your Name and Collaborator G and Collaborator H},
  journal={ACM Computing Surveys},
  volume={55},
  number={4},
  pages={1--38},
  year={2023},
  publisher={ACM}
}
```

---

*For a complete list of publications, please visit my [Google Scholar](https://scholar.google.com/) profile.*
